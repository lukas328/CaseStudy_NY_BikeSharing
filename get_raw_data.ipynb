{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cbb3a4",
   "metadata": {},
   "source": [
    "# Citibike Data Pipeline\n",
    "\n",
    "This notebook implements a workflow to download, extract, and convert Citibike ZIP files from an S3 bucket into a cleansed Parquet file. The pipeline follows a medallion architecture:\n",
    "- **01raw:** Contains the raw ZIP files and their extracted CSV files.\n",
    "- **02cleansed:** Contains the final cleansed Parquet file.\n",
    "\n",
    "Before downloading new data, the raw and cleansed folders are cleared so that only the current month's data remains.\n",
    "\n",
    "Process\n",
    "\n",
    "1. **Fetch the list of ZIP file links**  \n",
    "   Retrieve the list of available ZIP files from the base URL.\n",
    "\n",
    "2. **Determine the target ZIP file**\n",
    "   - If a specific timestamp is provided, look for it in the filename.\n",
    "   - Otherwise, select the latest available file.\n",
    "\n",
    "3. **Check if the target file is already present**\n",
    "   - If the file exists and `force_download` is set to `False`, skip the download.\n",
    "   - If not, clear the `raw` and `cleansed` folders to ensure only new data remains.\n",
    "\n",
    "4. **Download the ZIP file**  \n",
    "   Save the file into the `raw` folder.\n",
    "\n",
    "5. **Extract its contents**  \n",
    "   Extract the ZIP file into a designated subfolder.\n",
    "\n",
    "6. **Process the extracted CSV file**\n",
    "   - Locate the CSV file in the extracted contents.\n",
    "   - Convert it to a Parquet file and save it in the `cleansed` folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef8ab4",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f97bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd41786",
   "metadata": {},
   "source": [
    "## Step 2: Define a Helper Function to Clear Folders\n",
    "\n",
    "The `clear_folder` function deletes all files and subdirectories within a specified folder, neccessary to ensure that only one month of data is saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e466a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_folder(folder):\n",
    "    \"\"\"\n",
    "    Remove all files and subdirectories in the given folder.\n",
    "\n",
    "    This function ensures that the folder is empty. We use it to clean the \n",
    "    raw and cleansed folders before a new download, ensuring that only the new\n",
    "    month's data remains.\n",
    "    \"\"\"\n",
    "    if os.path.exists(folder):\n",
    "        for filename in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)  # Remove the file or link\n",
    "                elif os.path.isdir(file_path):\n",
    "                    import shutil\n",
    "                    shutil.rmtree(file_path)  # Recursively delete directories\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to delete {file_path}. Reason: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9452f11",
   "metadata": {},
   "source": [
    "## Step 3: Define Core Functions\n",
    "\n",
    "Core functions that:\n",
    "- **get_zip_links:** Use Selenium and BeautifulSoup to fetch all ZIP file links from the given URL.\n",
    "- **extract_latest_file:** Choose the latest ZIP file based on the date found in the filename.\n",
    "- **download_file:** Download a file from a URL (if it doesn't already exist).\n",
    "- **extract_zip_file:** Extract the contents of the ZIP file.\n",
    "- **convert_csv_to_parquet:** Convert the extracted CSV file into a Parquet file.\n",
    "- **get_newest_data:** Tie all the above steps together into a full workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5df114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip_links(base_url, selenium_options=None):\n",
    "    \"\"\"\n",
    "    Fetch all ZIP file links from the given URL using Selenium.\n",
    "\n",
    "    We load the page using Selenium (to handle dynamic content) and then use\n",
    "    BeautifulSoup to find links ending with '.zip'. If a link is relative,\n",
    "    we prepend the S3 bucket URL.\n",
    "    \"\"\"\n",
    "    options = selenium_options or webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(base_url)\n",
    "        time.sleep(3)  # Wait for the page to load\n",
    "        page_source = driver.page_source\n",
    "        driver.quit()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while fetching the page: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.zip')]\n",
    "    return [link if link.startswith(\"http\") else f\"https://s3.amazonaws.com/tripdata/{link}\" for link in links]\n",
    "\n",
    "def extract_latest_file(links):\n",
    "    \"\"\"\n",
    "    From a list of ZIP file links, select the one with the most recent date.\n",
    "\n",
    "    This function uses a regular expression to extract a date from each link,\n",
    "    sorts the links by date (in descending order), and returns the first (latest) link.\n",
    "    \"\"\"\n",
    "    def extract_date(link):\n",
    "        match = re.search(r\"(\\d{4}(?:\\d{2}){1,2})\", link)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    links_with_dates = [(link, extract_date(link)) for link in links]\n",
    "    links_with_dates = [item for item in links_with_dates if item[1]]\n",
    "    if not links_with_dates:\n",
    "        logging.warning(\"No valid dates found in the file links.\")\n",
    "        return None\n",
    "\n",
    "    links_with_dates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return links_with_dates[0][0]\n",
    "\n",
    "def download_file(url, folder):\n",
    "    \"\"\"\n",
    "    Download a file from a given URL and save it into the specified folder.\n",
    "\n",
    "    If the file already exists, the download is skipped.\n",
    "    \"\"\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    filename = os.path.join(folder, url.split(\"/\")[-1])\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        logging.info(f\"File already exists: {filename}. Skipping download.\")\n",
    "        return filename\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        logging.info(f\"Downloaded: {filename}\")\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_zip_file(zip_path, extract_to):\n",
    "    \"\"\"\n",
    "    Extract a ZIP file to the specified folder.\n",
    "\n",
    "    Returns a list of file paths that were extracted.\n",
    "    \"\"\"\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "            logging.info(f\"Extracted {zip_path} to {extract_to}\")\n",
    "            return [os.path.join(extract_to, file) for file in zip_ref.namelist()]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to extract {zip_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def convert_csv_to_parquet(csv_file, parquet_path):\n",
    "    \"\"\"\n",
    "    Read a CSV file and convert it to Parquet format.\n",
    "\n",
    "    All columns are initially read as strings to handle mixed types, and columns\n",
    "    ending with '_id' are optionally converted to numeric. The resulting DataFrame\n",
    "    is saved as a Parquet file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, dtype=str, low_memory=False)\n",
    "        logging.info(f\"Loaded CSV file: {csv_file}\")\n",
    "\n",
    "        # Convert columns ending with '_id' to numeric, if possible\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                if col.endswith(\"_id\"):\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to convert column {col}: {e}\")\n",
    "\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "        logging.info(f\"Converted {csv_file} to Parquet: {parquet_path}\")\n",
    "        return parquet_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to convert {csv_file} to Parquet: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154e299",
   "metadata": {},
   "source": [
    "## Step 4: Execution Function\n",
    "1. **Fetch the list of ZIP file links**  \n",
    "2. **Determine the target ZIP file**\n",
    "3. **Check if the target file is already present**\n",
    "4. **Download the ZIP file** \n",
    "5. **Extract its contents**  \n",
    "6. **Process the extracted CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d081eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_newest_data(\n",
    "    base_url,\n",
    "    raw_folder=\"01raw\",\n",
    "    extract_subfolder=\"csv_files\",\n",
    "    cleaned_folder=\"02cleansed\",\n",
    "    parquet_file=\"cleaned_data.parquet\",\n",
    "    timestamp=None,         # Use \"latest\" or None for the newest file; otherwise, provide a specific timestamp string\n",
    "    force_download=False    # If True, force a new download even if data exists\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    1. Fetch the list of ZIP file links from the base URL.\n",
    "    2. Determine the target ZIP file:\n",
    "       - If a specific timestamp is provided, look for it in the filename.\n",
    "       - Otherwise, select the latest available file.\n",
    "    3. Check if the target file is already present:\n",
    "       - If it is and force_download is False, skip the download.\n",
    "       - Otherwise, clear the raw and cleansed folders so only the new data remains.\n",
    "    4. Download the ZIP file into the raw folder.\n",
    "    5. Extract its contents into a designated subfolder.\n",
    "    6. Find the CSV file in the extracted files and convert it to a Parquet file in the cleansed folder.\n",
    "    \"\"\"\n",
    "    # Ensure that the raw and cleansed folders exist\n",
    "    os.makedirs(raw_folder, exist_ok=True)\n",
    "    os.makedirs(cleaned_folder, exist_ok=True)\n",
    "    extract_folder = os.path.join(raw_folder, extract_subfolder)\n",
    "    os.makedirs(extract_folder, exist_ok=True)\n",
    "    \n",
    "    logging.info(\"Fetching ZIP file links...\")\n",
    "    zip_links = get_zip_links(base_url)\n",
    "    if not zip_links:\n",
    "        logging.error(\"No ZIP files found.\")\n",
    "        return None\n",
    "\n",
    "    # Select the target ZIP file based on the timestamp (or default to latest)\n",
    "    if timestamp and timestamp.lower() != \"latest\":\n",
    "        target_link = None\n",
    "        for link in zip_links:\n",
    "            if timestamp in link:\n",
    "                target_link = link\n",
    "                break\n",
    "        if not target_link:\n",
    "            logging.error(f\"No file found with timestamp {timestamp}.\")\n",
    "            return None\n",
    "    else:\n",
    "        target_link = extract_latest_file(zip_links)\n",
    "        if not target_link:\n",
    "            logging.error(\"Could not determine the latest file.\")\n",
    "            return None\n",
    "\n",
    "    logging.info(f\"Target ZIP file: {target_link.split('/')[-1]}\")\n",
    "\n",
    "    # Define local file paths for the ZIP and final Parquet file\n",
    "    local_zip_path = os.path.join(raw_folder, target_link.split(\"/\")[-1])\n",
    "    parquet_path = os.path.join(cleaned_folder, parquet_file)\n",
    "\n",
    "    # If data already exists (and force_download is False), skip download;\n",
    "    # otherwise, clear the folders so that only the new data will remain.\n",
    "    if not force_download and os.path.exists(local_zip_path) and os.path.exists(parquet_path):\n",
    "        logging.info(\"Data already downloaded and cleansed. Skipping download.\")\n",
    "        return parquet_path\n",
    "    else:\n",
    "        logging.info(\"Clearing raw and cleansed folders for new data download...\")\n",
    "        clear_folder(raw_folder)\n",
    "        clear_folder(cleaned_folder)\n",
    "        # Recreate the extraction folder after clearing the raw folder\n",
    "        os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "    # Download the ZIP file\n",
    "    downloaded_file = download_file(target_link, raw_folder)\n",
    "    if not downloaded_file:\n",
    "        return None\n",
    "\n",
    "    # Extract the ZIP file into the extraction folder\n",
    "    extracted_files = extract_zip_file(downloaded_file, extract_to=extract_folder)\n",
    "    if not extracted_files:\n",
    "        return None\n",
    "\n",
    "    # Convert the first CSV file found in the extracted files to Parquet\n",
    "    for file in extracted_files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            logging.info(f\"Converting {file} to Parquet...\")\n",
    "            return convert_csv_to_parquet(file, parquet_path)\n",
    "\n",
    "    logging.error(\"No CSV file found in the extracted files.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7849c6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Data pipeline for the Citibike dataset:\n",
    "- **01raw:** Holds the raw ZIP and CSV files (cleared before each new download).\n",
    "- **02cleansed:** Contains the final cleansed Parquet file (also cleared before each new download).\n",
    "\n",
    "Each time you run the notebook, it checks for new data (or a specific timestamp), clears the previous data, downloads, extracts, and converts the data .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
